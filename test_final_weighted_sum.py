#!/usr/bin/env python3
"""
å®Œå…¨ç‰ˆï¼šFeaturizerã«ã‚ˆã‚‹å…¨25å±¤ã®é‡ã¿ä»˜ãå’Œã‚’ä½¿ç”¨
"""

import torch
import librosa
import argparse
import json
from datetime import datetime
from transformers import HubertModel
import warnings
warnings.filterwarnings('ignore')

torch.serialization.add_safe_globals([argparse.Namespace])

class KushinadaFinalImplementation:
    LABEL_MAP = {
        0: "neutral",
        1: "joy",
        2: "anger",
        3: "sadness"
    }

    def __init__(self):
        self.upstream = None
        self.featurizer_weights = None
        self.projector = None
        self.post_net = None
        self.loaded = False

    def load_models(self):
        print("ğŸ”§ Upstreamãƒ¢ãƒ‡ãƒ«ï¼ˆHuBERT Largeï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        self.upstream = HubertModel.from_pretrained("imprt/kushinada-hubert-large")
        self.upstream.eval()

        print("ğŸ”§ Downstreamãƒ¢ãƒ‡ãƒ«ï¼ˆæ„Ÿæƒ…åˆ†é¡ï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        checkpoint_path = "./checkpoints/models--imprt--kushinada-hubert-large-jtes-er/snapshots/2ff7d7337e9b67d695193a38a8fe01639de9fe58/s3prl/result/downstream/kushinada-hubert-large-jtes-er_fold1/dev-best.ckpt"
        downstream_ckpt = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

        # Featurizerï¼ˆå…¨å±¤ã®é‡ã¿ä»˜ãå’Œï¼‰
        if 'Featurizer' in downstream_ckpt:
            self.featurizer_weights = downstream_ckpt['Featurizer']['weights']
            print(f"   Featurizer weights: {self.featurizer_weights.shape}")

        downstream_dict = downstream_ckpt["Downstream"]

        projector_weight = downstream_dict["projector.weight"]
        self.projector = torch.nn.Linear(projector_weight.size(1), projector_weight.size(0))
        self.projector.load_state_dict({"weight": projector_weight, "bias": downstream_dict["projector.bias"]})
        self.projector.eval()

        post_net_weight = downstream_dict["model.post_net.linear.weight"]
        self.post_net = torch.nn.Linear(post_net_weight.size(1), post_net_weight.size(0))
        self.post_net.load_state_dict({"weight": post_net_weight, "bias": downstream_dict["model.post_net.linear.bias"]})
        self.post_net.eval()

        self.loaded = True
        print("âœ… ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†\n")

    def weighted_sum_layers(self, all_hidden_states):
        """
        å…¨å±¤ã®é‡ã¿ä»˜ãå’Œã‚’è¨ˆç®—
        all_hidden_states: list of [batch, time, hidden_dim] (25å±¤åˆ†)
        """
        # é‡ã¿ã‚’softmaxã§æ­£è¦åŒ–
        norm_weights = torch.softmax(self.featurizer_weights, dim=0)

        # å…¨å±¤ã‚’ã‚¹ã‚¿ãƒƒã‚¯: [25, batch, time, hidden_dim]
        stacked = torch.stack(all_hidden_states, dim=0)

        # é‡ã¿ä»˜ãå’Œ: [batch, time, hidden_dim]
        weighted = (stacked * norm_weights.view(-1, 1, 1, 1)).sum(dim=0)

        return weighted

    def predict_emotion(self, audio_path):
        if not self.loaded:
            self.load_models()

        print(f"ğŸ“ éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {audio_path}")

        # librosaã§èª­ã¿è¾¼ã¿
        waveform_np, sample_rate = librosa.load(audio_path, sr=16000, mono=True)
        waveform = torch.from_numpy(waveform_np).float().unsqueeze(0)

        duration = len(waveform_np) / 16000
        print(f"   é•·ã•: {duration:.2f}ç§’")

        print("\nğŸµ æ„Ÿæƒ…åˆ†æã‚’å®Ÿè¡Œä¸­...")
        with torch.no_grad():
            # HuBERTã§å…¨å±¤ã®ç‰¹å¾´æŠ½å‡º
            outputs = self.upstream(
                waveform,
                output_hidden_states=True  # å…¨å±¤ã‚’å–å¾—
            )

            # all_hidden_states: tuple of 25 tensors, each [1, time, 1024]
            all_hidden_states = outputs.hidden_states
            print(f"   å…¨å±¤æ•°: {len(all_hidden_states)}")
            print(f"   å„å±¤shape: {all_hidden_states[0].shape}")

            # Featurizerã§å…¨å±¤ã®é‡ã¿ä»˜ãå’Œ
            if self.featurizer_weights is not None:
                features = self.weighted_sum_layers(all_hidden_states)  # [1, time, 1024]
                print(f"   é‡ã¿ä»˜ãå’Œå¾Œ: {features.shape}")
            else:
                # FeaturizerãŒãªã„å ´åˆã¯æœ€çµ‚å±¤ã®ã¿
                features = outputs.last_hidden_state
                print(f"   æœ€çµ‚å±¤ã®ã¿: {features.shape}")

            # æ™‚é–“æ–¹å‘ã«å¹³å‡ï¼ˆMeanPoolingï¼‰
            pooled = features.mean(dim=1)  # [1, 1024]
            print(f"   ãƒ—ãƒ¼ãƒªãƒ³ã‚°å¾Œ: {pooled.shape}")

            # Projector â†’ Classifier
            projected = self.projector(pooled)  # [1, 256]
            logits = self.post_net(projected)  # [1, 4]
            probs = torch.softmax(logits, dim=-1)

        # çµæœ
        logits_np = logits[0].numpy()
        probs_np = probs[0].numpy()
        predicted_class = probs_np.argmax()

        print(f"\nğŸ“Š ç”Ÿã®logits: {logits_np}")
        print(f"   ç¯„å›²: {logits_np.max() - logits_np.min():.4f}")
        print(f"   æ¨™æº–åå·®: {logits_np.std():.4f}")

        emotion_scores = {self.LABEL_MAP[i]: float(probs_np[i]) for i in range(4)}
        dominant_emotion = self.LABEL_MAP[predicted_class]
        confidence = float(probs_np[predicted_class])

        return {
            "audio_file": audio_path,
            "duration_seconds": duration,
            "dominant_emotion": dominant_emotion,
            "confidence": confidence,
            "all_emotions": emotion_scores,
            "logits_range": float(logits_np.max() - logits_np.min()),
            "logits_std": float(logits_np.std())
        }

def main():
    print("=" * 80)
    print("ğŸ¯ Kushinadaæ„Ÿæƒ…èªè­˜ï¼ˆå®Œå…¨ç‰ˆ - å…¨å±¤é‡ã¿ä»˜ãå’Œï¼‰")
    print("=" * 80)
    print()

    test_audio = "/Users/kaya.matsumoto/Desktop/ja_anger_netflix_001.wav"
    recognizer = KushinadaFinalImplementation()
    result = recognizer.predict_emotion(test_audio)

    print("\n" + "=" * 80)
    print("ğŸ“Š åˆ†æçµæœ")
    print("=" * 80)

    sorted_emotions = sorted(result['all_emotions'].items(), key=lambda x: x[1], reverse=True)

    print("\næ„Ÿæƒ…ã‚¹ã‚³ã‚¢ï¼ˆé™é †ï¼‰:")
    print("-" * 60)

    for i, (emotion, score) in enumerate(sorted_emotions, 1):
        bar_length = int(score * 50)
        bar = "â–ˆ" * bar_length
        marker = "ğŸ‘‘" if i == 1 else "  "
        print(f"{marker} {i}. {emotion:10} : {score:6.2%} {bar}")

    print("-" * 60)
    print(f"\nğŸ­ ä¸»è¦æ„Ÿæƒ…: {result['dominant_emotion']} (ä¿¡é ¼åº¦: {result['confidence']:.2%})")
    print(f"\nğŸ“ˆ è¨ºæ–­æŒ‡æ¨™:")
    print(f"   logitsç¯„å›²: {result['logits_range']:.4f} (æ­£å¸¸: > 1.0)")
    print(f"   logitsæ¨™æº–åå·®: {result['logits_std']:.4f} (æ­£å¸¸: > 0.5)")

    if result['logits_range'] > 1.0 and result['logits_std'] > 0.5:
        print("\nâœ… ãƒ¢ãƒ‡ãƒ«ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼")
    else:
        print("\nâš ï¸  ã¾ã å•é¡ŒãŒæ®‹ã£ã¦ã„ã¾ã™...")

    output = {
        "model": "kushinada-hubert-large-jtes-er (final weighted sum)",
        "timestamp": datetime.now().isoformat(),
        **result
    }

    output_file = "test_final_weighted_sum_result.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜: {output_file}")
    print("\n" + "=" * 80)
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
    print("=" * 80)

if __name__ == "__main__":
    main()
