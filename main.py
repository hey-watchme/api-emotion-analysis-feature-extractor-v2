#!/usr/bin/env python3
"""
KushinadaéŸ³å£°æ„Ÿæƒ…èªè­˜API - OpenSMILEäº’æ›ç‰ˆ
æ—¥æœ¬èªéŸ³å£°ã®æ„Ÿæƒ…èªè­˜ã‚’è¡Œã†APIã‚µãƒ¼ãƒ“ã‚¹
ç”£ç·ç ”ã®Kushinadaãƒ¢ãƒ‡ãƒ«ï¼ˆHuBERT-largeï¼‰ã‚’ä½¿ç”¨
"""

import os
import gc
import time
import tempfile
import torch
import librosa
import numpy as np
import warnings
from transformers import HubertModel
import boto3
from botocore.exceptions import ClientError
from dotenv import load_dotenv
from supabase import create_client, Client

from fastapi import FastAPI, HTTPException, status
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from typing import Dict, List, Optional

from models import (
    HealthResponse,
    ErrorResponse,
    EmotionFeaturesRequest,
    EmotionFeaturesResponse,
    ChunkResult,
    EmotionScore
)
from supabase_service import SupabaseService

warnings.filterwarnings('ignore')

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–
app = FastAPI(
    title="Kushinada Emotion Recognition API - OpenSMILE Compatible",
    description="Kushinadaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸfile_pathsãƒ™ãƒ¼ã‚¹ã®æ„Ÿæƒ…åˆ†æã‚µãƒ¼ãƒ“ã‚¹",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã®è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_KEY")
if supabase_url and supabase_key:
    supabase_client: Client = create_client(supabase_url, supabase_key)
    supabase_service = SupabaseService(supabase_client)
    print(f"âœ… Supabaseæ¥ç¶šè¨­å®šå®Œäº†: {supabase_url}")
else:
    supabase_service = None
    print("âš ï¸ Supabaseç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

# AWS S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
s3_bucket_name = os.getenv('S3_BUCKET_NAME', 'watchme-vault')
aws_region = os.getenv('AWS_REGION', 'us-east-1')

if not aws_access_key_id or not aws_secret_access_key:
    raise ValueError("AWS_ACCESS_KEY_IDãŠã‚ˆã³AWS_SECRET_ACCESS_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

s3_client = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=aws_region
)
print(f"âœ… AWS S3æ¥ç¶šè¨­å®šå®Œäº†: ãƒã‚±ãƒƒãƒˆ={s3_bucket_name}, ãƒªãƒ¼ã‚¸ãƒ§ãƒ³={aws_region}")

# ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¨­å®š
SEGMENT_DURATION = 10.0  # 10ç§’å›ºå®šï¼ˆæœ€é©ãƒãƒ©ãƒ³ã‚¹ç¢ºèªæ¸ˆã¿ï¼‰

# Kushinadaæ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã®è©³ç´°æƒ…å ±ï¼ˆ4æ„Ÿæƒ…ï¼‰
LABELS_INFO = {
    "neutral": {"ja": "ä¸­ç«‹", "en": "Neutral", "group": "neutral"},
    "joy": {"ja": "å–œã³", "en": "Joy", "group": "positive_active"},
    "anger": {"ja": "æ€’ã‚Š", "en": "Anger", "group": "negative_active"},
    "sadness": {"ja": "æ‚²ã—ã¿", "en": "Sadness", "group": "negative_passive"}
}

LABEL_MAP = {
    0: "neutral",
    1: "joy",
    2: "anger",
    3: "sadness"
}


class KushinadaAnalyzer:
    """Kushinadaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ„Ÿæƒ…åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.upstream = None
        self.featurizer_weights = None
        self.projector = None
        self.post_net = None
        self.loaded = False

    def load_models(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if self.loaded:
            return

        print("ğŸ”§ Kushinadaãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")

        # HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š
        hf_token = os.getenv('HF_TOKEN')
        if hf_token:
            os.environ['HUGGING_FACE_HUB_TOKEN'] = hf_token

        # HuBERT ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        self.upstream = HubertModel.from_pretrained("imprt/kushinada-hubert-large")
        self.upstream.eval()

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ï¼ˆHugging Faceã‹ã‚‰è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
        from huggingface_hub import hf_hub_download

        checkpoint_path = hf_hub_download(
            repo_id="imprt/kushinada-hubert-large-jtes-er",
            filename="s3prl/result/downstream/kushinada-hubert-large-jtes-er_fold1/dev-best.ckpt",
            token=os.getenv("HF_TOKEN")
        )

        print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {checkpoint_path}")

        downstream_ckpt = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

        # Featurizer weightsï¼ˆå…¨25å±¤ã®é‡ã¿ï¼‰
        if 'Featurizer' in downstream_ckpt:
            self.featurizer_weights = downstream_ckpt['Featurizer']['weights']

        downstream_dict = downstream_ckpt["Downstream"]

        # Projectorï¼ˆ1024æ¬¡å…ƒ â†’ 256æ¬¡å…ƒï¼‰
        projector_weight = downstream_dict["projector.weight"]
        self.projector = torch.nn.Linear(projector_weight.size(1), projector_weight.size(0))
        self.projector.load_state_dict({
            "weight": projector_weight,
            "bias": downstream_dict["projector.bias"]
        })
        self.projector.eval()

        # Post-netï¼ˆClassifier: 256æ¬¡å…ƒ â†’ 4æ¬¡å…ƒï¼‰
        post_net_weight = downstream_dict["model.post_net.linear.weight"]
        self.post_net = torch.nn.Linear(post_net_weight.size(1), post_net_weight.size(0))
        self.post_net.load_state_dict({
            "weight": post_net_weight,
            "bias": downstream_dict["model.post_net.linear.bias"]
        })
        self.post_net.eval()

        self.loaded = True
        print("âœ… Kushinadaãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\n")

    def weighted_sum_layers(self, all_hidden_states):
        """å…¨25å±¤ã®é‡ã¿ä»˜ãå’Œã‚’è¨ˆç®—ï¼ˆFeaturizerï¼‰"""
        norm_weights = torch.softmax(self.featurizer_weights, dim=0)
        stacked = torch.stack(all_hidden_states, dim=0)
        weighted = (stacked * norm_weights.view(-1, 1, 1, 1)).sum(dim=0)
        return weighted

    def predict_segment(self, waveform_segment):
        """
        å˜ä¸€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ„Ÿæƒ…åˆ†æ

        Args:
            waveform_segment: torch.Tensor [samples]

        Returns:
            dict: æ„Ÿæƒ…åˆ†æçµæœ
        """
        if len(waveform_segment) < 1600:  # 0.1ç§’æœªæº€ã¯å‡¦ç†ã—ãªã„
            return None

        waveform = waveform_segment.unsqueeze(0)  # [1, samples]

        with torch.no_grad():
            # HuBERT: å…¨25å±¤ã‚’å–å¾—
            outputs = self.upstream(waveform, output_hidden_states=True)
            all_hidden_states = outputs.hidden_states

            # Featurizer: å…¨å±¤ã®é‡ã¿ä»˜ãå’Œ
            if self.featurizer_weights is not None:
                features = self.weighted_sum_layers(all_hidden_states)
            else:
                features = outputs.last_hidden_state

            # MeanPooling
            pooled = features.mean(dim=1)

            # Projector â†’ Classifier
            projected = self.projector(pooled)
            logits = self.post_net(projected)

        # logitsã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆæƒ…å ±åŠ£åŒ–ã‚’é˜²ããŸã‚ã€softmaxã¯é©ç”¨ã—ãªã„ï¼‰
        logits_np = logits[0].numpy()
        predicted_class = logits_np.argmax()

        # 4æ„Ÿæƒ…ã™ã¹ã¦ã®logitsã‚’å–å¾—ï¼ˆç”Ÿã‚¹ã‚³ã‚¢: -âˆï½+âˆã®ç¯„å›²ï¼‰
        emotion_scores = {LABEL_MAP[i]: float(logits_np[i]) for i in range(4)}
        dominant_emotion = LABEL_MAP[predicted_class]
        confidence = float(logits_np[predicted_class])

        return {
            "dominant_emotion": dominant_emotion,
            "confidence": confidence,
            "all_emotions": emotion_scores
        }

    def analyze_audio_file(self, audio_path: str) -> tuple:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’10ç§’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²ã—ã¦æ„Ÿæƒ…åˆ†æ

        Args:
            audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

        Returns:
            tuple: (ã‚»ã‚°ãƒ¡ãƒ³ãƒˆçµæœãƒªã‚¹ãƒˆ, ç·æ™‚é–“)
        """
        if not self.loaded:
            self.load_models()

        # éŸ³å£°èª­ã¿è¾¼ã¿ï¼ˆ16kHzã€ãƒ¢ãƒãƒ©ãƒ«ï¼‰
        waveform_np, sample_rate = librosa.load(audio_path, sr=16000, mono=True)
        total_duration = len(waveform_np) / 16000

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«åˆ†å‰²
        segment_samples = int(SEGMENT_DURATION * 16000)
        num_segments = int(np.ceil(len(waveform_np) / segment_samples))

        chunks_results = []

        for i in range(num_segments):
            chunk_id = i + 1
            start_sample = i * segment_samples
            end_sample = min((i + 1) * segment_samples, len(waveform_np))

            start_time = start_sample / 16000
            end_time = end_sample / 16000
            duration = end_time - start_time

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º
            segment_waveform = torch.from_numpy(waveform_np[start_sample:end_sample]).float()

            # æ„Ÿæƒ…åˆ†æå®Ÿè¡Œ
            result = self.predict_segment(segment_waveform)

            if result:
                # 4æ„Ÿæƒ…ã™ã¹ã¦ã‚’emotionsé…åˆ—ã«æ•´å½¢
                emotions = []
                for label_id in range(4):
                    label = LABEL_MAP[label_id]
                    score = result["all_emotions"][label]
                    info = LABELS_INFO[label]

                    emotions.append({
                        "label": label,
                        "score": round(score, 6),  # logitsç”Ÿã‚¹ã‚³ã‚¢ï¼ˆ-âˆï½+âˆï¼‰
                        "name_ja": info["ja"],
                        "name_en": info["en"],
                        "group": info["group"]
                    })

                # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                emotions.sort(key=lambda x: x["score"], reverse=True)

                # ãƒãƒ£ãƒ³ã‚¯çµæœã‚’ä½œæˆ
                chunk_result = {
                    "chunk_id": chunk_id,
                    "start_time": round(start_time, 1),
                    "end_time": round(end_time, 1),
                    "duration": round(duration, 1),
                    "emotions": emotions,
                    "primary_emotion": emotions[0] if emotions else None
                }

                chunks_results.append(chunk_result)

            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            del segment_waveform
            gc.collect()

        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del waveform_np
        gc.collect()

        return chunks_results, int(total_duration)


# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’ä¿æŒ
kushinada_analyzer = None




# èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
@app.on_event("startup")
async def startup_event():
    global kushinada_analyzer
    kushinada_analyzer = KushinadaAnalyzer()
    kushinada_analyzer.load_models()


@app.get("/", response_model=dict)
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "Kushinada Emotion Recognition API - OpenSMILE Compatible",
        "version": "2.0.0",
        "model": "kushinada-hubert-large-jtes-er",
        "segment_duration": f"{SEGMENT_DURATION}ç§’",
        "emotions": list(LABELS_INFO.keys()),
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        return HealthResponse(
            status="healthy",
            service="Kushinada API - OpenSMILE Compatible",
            version="2.0.0",
            model_loaded=kushinada_analyzer is not None and kushinada_analyzer.loaded
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Service unhealthy: {str(e)}"
        )


@app.post("/process/emotion-features", response_model=EmotionFeaturesResponse)
async def process_emotion_features(request: EmotionFeaturesRequest):
    """file_paths-based emotion analysis (spot_features table with UTC timestamp)"""
    start_time = time.time()

    try:
        print(f"\n=== Kushinada Emotion Analysis Start (UTC-based architecture) ===")
        print(f"file_paths: {len(request.file_paths)} files to process")
        print(f"Segment duration: {SEGMENT_DURATION} seconds")
        print(f"=" * 50)

        if not supabase_service:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Supabase service unavailable. Check environment variables."
            )

        if not kushinada_analyzer or not kushinada_analyzer.loaded:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Kushinada model not loaded."
            )

        processed_files = 0
        error_files = []

        with tempfile.TemporaryDirectory() as temp_dir:
            for file_path in request.file_paths:
                try:
                    print(f"\nğŸ“¥ Fetching file from S3: {file_path}")

                    # Get device_id and recorded_at from audio_files table
                    audio_file_response = supabase_client.table('audio_files') \
                        .select('device_id, recorded_at') \
                        .eq('file_path', file_path) \
                        .single() \
                        .execute()

                    if not audio_file_response.data:
                        print(f"âš ï¸ Audio file record not found: {file_path}")
                        error_files.append(file_path)
                        continue

                    device_id = audio_file_response.data['device_id']
                    recorded_at = audio_file_response.data['recorded_at']

                    # Update status to processing
                    await supabase_service.update_audio_files_status(file_path, 'processing')

                    # Download from S3
                    temp_file_path = os.path.join(temp_dir, f"{device_id}_{recorded_at}.wav")

                    try:
                        s3_client.download_file(s3_bucket_name, file_path, temp_file_path)
                        print(f"âœ… S3 download success: {file_path}")
                    except ClientError as e:
                        error_code = e.response['Error']['Code']
                        if error_code == 'NoSuchKey':
                            print(f"âš ï¸ File not found: {file_path}")
                            await supabase_service.update_audio_files_status(file_path, 'error')
                            error_files.append(file_path)
                            continue
                        else:
                            raise e

                    print(f"ğŸµ Kushinada emotion analysis start: {file_path}")

                    # Run emotion analysis
                    analysis_start = time.time()
                    chunks_results, duration_seconds = kushinada_analyzer.analyze_audio_file(temp_file_path)
                    processing_time = time.time() - analysis_start

                    # Save to spot_features table
                    save_success = await supabase_service.save_to_spot_features(
                        device_id,
                        recorded_at,
                        chunks_results
                    )

                    if save_success:
                        # Update status to completed
                        await supabase_service.update_audio_files_status(file_path, 'completed')
                        processed_files += 1

                        # Display primary emotions
                        if chunks_results:
                            for chunk in chunks_results:
                                primary = chunk["primary_emotion"]
                                print(f"  Segment {chunk['chunk_id']}: {primary['name_ja']} (score: {primary['score']:.2f})")

                        print(f"âœ… Completed: {file_path} â†’ {len(chunks_results)} segments analyzed")
                    else:
                        await supabase_service.update_audio_files_status(file_path, 'error')
                        error_files.append(file_path)

                except Exception as e:
                    error_files.append(file_path)
                    print(f"âŒ Error: {file_path} - {str(e)}")

                    # Save error to spot_features
                    try:
                        audio_file_response = supabase_client.table('audio_files') \
                            .select('device_id, recorded_at') \
                            .eq('file_path', file_path) \
                            .single() \
                            .execute()

                        if audio_file_response.data:
                            await supabase_service.save_to_spot_features(
                                audio_file_response.data['device_id'],
                                audio_file_response.data['recorded_at'],
                                [],
                                error=str(e)
                            )
                            await supabase_service.update_audio_files_status(file_path, 'error')
                    except:
                        pass

        # Response
        total_time = time.time() - start_time

        print(f"\n=== Kushinada Emotion Analysis Complete ===")
        print(f"ğŸ“¥ S3 processing: {processed_files} files")
        print(f"âŒ Errors: {len(error_files)} files")
        print(f"â±ï¸ Total processing time: {total_time:.2f} seconds")
        print(f"=" * 50)

        return EmotionFeaturesResponse(
            success=True,
            processed_files=processed_files,
            saved_count=processed_files,
            error_files=error_files,
            total_processing_time=total_time,
            message=f"Processed {processed_files} files from S3 and saved to spot_features table"
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error during emotion analysis: {str(e)}"
        )


@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content=ErrorResponse(
            error="Internal server error",
            detail=str(exc)
        ).dict()
    )


if __name__ == "__main__":
    # ãƒãƒ¼ãƒˆ8018ã§èµ·å‹•ï¼ˆv3ã¨åŒã˜ãƒãƒ¼ãƒˆï¼‰
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8018,
        reload=True,
        log_level="info"
    )
